\documentclass[]{article}
\usepackage{graphicx}

\begin{document}

\title{Classifying positive and unlabeled training  with multinomial logistic regression}
\author{Joseph Perla}
\date{1/31/2014}
\maketitle

\section{The Probem}

In a common supervised learning task, we are given a training set where all samples are labeled either positive or negative. Semi-supervised algorithms operate on input training sets with some positive and negative labels, and some unlabeled data.  We studied the positive-only problem where the training set has some positive labels, no negative labels, and the remainder unlabeled.  In this paper, we assume that the labeled positive samples are selected uniformly at random from the distribution of all positive samples with probability $c$. Therefore, the labeled and unlabeled positive samples have identical distributions.

\section{The Multinomial Regression Model}

We model the positive-only problem with a multinomial regression in the following way: assume there are 3 classes.  One class is positive and labeled ($P_L$), one is positive and unlabeled ($P_U$), and the last is negative and unlabeled (N).  There are no negative labeled samples.  We use the standard multinomial regression probabilities, but we share the weights $w_p$ between the positive-labeled ($P_L$) and positive-unlabeled ($P_U$) classes. Finally, we include a bias term $b$ which biases the constant fraction of the positive samples which are labeled.  $Z_N$ is the normalization factor:

$$
p(k=P_L | x, w_p, w_n) =  \frac{e^{w_p \cdot x}}{Z_N} = \frac{e^{w_p \cdot x}}{e^{w_p \cdot x} + \cdot e^{w_p \cdot x + b} + e^{w_n \cdot x}} = \frac{e^{w_p \cdot x}}{(1 + e^{b}) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}}
$$

$$
p(k=P_U | x, w_p, w_n) =  \frac{e^{w_p \cdot x + b}}{Z_N} = \frac{e^{w_p \cdot x + b}}{e^{w_p \cdot x} + e^{w_p \cdot x + b} + e^{w_n \cdot x}} = \frac{e^{w_p \cdot x + b}}{(1 + e^{b}) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}}
$$

$$
p(k=N | x, w_p, w_n) =  \frac{e^{w_n \cdot x}}{Z_N} = \frac{e^{w_n \cdot x}}{e^{w_p \cdot x} + e^{w_p \cdot x + b} + e^{w_n \cdot x}} = \frac{e^{w_n \cdot x}}{(1 + e^{b}) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}}
$$

Notice that, $\forall x$,

$$
\frac{p(k=P_L | x, w_p, w_n)}{p(k=P_L | x, w_p, w_n) + p(k=P_U | x, w_p, w_n)} = \frac{1}{1 + e^b} = c.
$$

Thus, $c$ is the constant fraction of positive samples which are labeled.

Finally, we can divide by $e^{w_p \cdot x}$ through all the terms in order to consolidate parameters. Let $w = w_n - w_p$,

$$
p(k=P_L | x, w_p, w_n) = \frac{e^{w_p \cdot x}}{(1 + e^{b}) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}} = \frac{1}{1 + e^b + e^{w \cdot x}}
$$

$$
p(k=P_U | x, w_p, w_n) = \frac{e^{w_p \cdot x + b}}{(1 + e^{b}) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}} = \frac{e^b}{1 + e^b + e^{w \cdot x}}
$$

$$
p(k=N | x, w_p, w_n) = \frac{e^{w_n \cdot x}}{(1 + e^{b}) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}} = \frac{e^{w \cdot x}}{1 + e^b + e^{w \cdot x}}
$$

So we only need to learn the vector $w$.

\section{Learning $\beta = b, w$ from $D$}

Assuming we have independently drawn data $D$, the conditional log likelihood, where $\beta = c, w_p, w_n$ is
$$
\log{p(D | \beta)} = \log \prod_{j=1...n}{p(\ell_j, x_j | \beta)} = \sum_{j=1...n}{\log{p(\ell_j, x_j | \beta)}} = \sum_{j=1...n}{\log{p(\ell_j | x_j, \beta)}} \cdot p(x_j | \beta)
$$

where $\ell_j$ is an indicator variable describing whether the sample is labeled or not. $\ell_j = 1$ if and only if $k_j=P_L$, otherwise $k_j = N$ or $k_j=P_U$ and $\ell_j = 0$.  Also note that $p(x_j|\beta) = p(x_j)$ and we assume uninformative uniform prior for $x_j$, so let $p(x_j|\beta) = 1$.

The maximum likelihood (ML) estimate $\hat \beta$ is the value of $\beta$ maximizing the likelihood of the data $D$:
$$
\hat \beta = arg max_{\beta} p(D | \beta) = arg max_{\beta} \log{p(D | \beta)}.
$$

\subsection{Stochastic Gradient}
We want to use stochastic gradient following to learn the parameters $\beta$. So take the partial derivatives of the log conditional likelihood (LCL) for each parameter.

\subsubsection{$ \nabla_{w_i}{Err_R (D, \beta)}$}

$$
\nabla_{w_i}{Err_R (D, \beta)} = \frac{\partial}{\partial w_i} \log{p(D|\beta)} =  \sum_{j=1...n}{\frac{\partial}{\partial w_i}\log{p(\ell_j | x_j, \beta)}}
$$

so for each $j$,

$$
 = \frac{\partial}{\partial w_i}\log{p(\ell_j | x_j, \beta)}
$$

$$
 = \frac{\partial}{\partial w_i}
		\log{\Bigg(
			I(\ell_j=1) \cdot \left( \frac{1}{1 + e^b + e^{w \cdot x}} \right) + 
			I(\ell_j=0) \cdot \left(\frac{e^b}{1 + e^b + e^{w \cdot x}} + \frac{e^{w \cdot x}}{1 + e^b + e^{w \cdot x}} \right)
		\Bigg)}
$$

$$
 = \frac{\partial}{\partial w_i}
		\log{\Bigg(
		    \frac{
			I(\ell_j=1) + 
			I(\ell_j=0) \cdot \left( e^b + e^{w \cdot x} \right)
		     }{1 + e^b + e^{w \cdot x}}
		\Bigg)}
$$

$$
 = \frac{\partial}{\partial w_i}
		\log{\bigg(
			I(\ell_j=1) + 
			I(\ell_j=0) \cdot \left( e^b + e^{w \cdot x} \right)
		\bigg)}
		- \frac{\partial}{\partial w_i}
		 \log{\bigg(
			1 + e^b + e^{w \cdot x}
		\bigg)}
$$


$$
 = I(\ell_j=1) \cdot \frac{\partial}{\partial w_i}
		\log{(1)} +
    I(\ell_j=0) \cdot \frac{\partial}{\partial w_i}
		\log{ \left( e^b + e^{w \cdot x} \right) }
		- \frac{\partial}{\partial w_i}
		 \log{\bigg(
			1 + e^b + e^{w \cdot x}
		\bigg)}
$$

$$
 = I(\ell_j=0) \cdot \frac{\partial}{\partial w_i}
		\log{  \left( e^b + e^{w \cdot x} \right) }
		- \frac{\partial}{\partial w_i}
		 \log{\bigg(
			1 + e^b + e^{w \cdot x}
		\bigg)}
$$


$$
 = I(\ell_j=0) \cdot 
 		\frac{
			e^{w \cdot x}
		}{
			e^b + e^{w \cdot x}
		} \cdot x_{j,i}
		- \frac{\partial}{\partial w_i}
		 \log{\Big(
			1 + e^b + e^{w \cdot x}
		\Big)}
$$

$$
 = I(\ell_j=0) \cdot 
 		\frac{
			e^{w \cdot x}
		}{
			e^b + e^{w \cdot x}
		} \cdot x_{j,i}
		- \frac{e^{w \cdot x}}{
			1 + e^b + e^{w \cdot x}
		} \cdot x_{j,i}
$$

$$
 = x_{j,i} \cdot
    \Big(	I(\ell_j=0) \cdot p(k_j=N | \ell_j=0, x_j, \beta) - 
		p(k_j=N | x_j, \beta)
    \Big)
$$

\subsubsection{$ \nabla_{b}{Err_R (D, \beta)}$}


Similarly for the parameter $b$ in $\beta$, for each $j$,


$$
 = \frac{\partial}{\partial c}\log{p(\ell_j | x_j, \beta)}
$$


$$
 = I(\ell_j=0) \cdot \frac{\partial}{\partial b}
		\log{  \left( e^b + e^{w \cdot x} \right) }
		- \frac{\partial}{\partial b}
		 \log{\bigg(
			1 + e^b + e^{w \cdot x}
		\bigg)}
$$


$$
 = I(\ell_j=0) \cdot
		\frac{1}{e^b + e^{w \cdot x}}
		-
		 \frac{1}{
			1 + e^b + e^{w \cdot x}
		}
$$


$$
 = I(\ell_j=0) \cdot
		\frac{p(\ell_j=1 | x_j,\beta)}{p(\ell_j=0 | x_j,\beta)}
		-
		 p(\ell_j=1 | x_j,\beta)
$$

\section{Results}

This was implemented in Python and numpy.

\label{Reproduce Elkan 2008 graph with generated data (c=0.20)}
\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{newgraph.png}
\caption{The deep blue marks a classifier trained using the true labels. It represents the best possible classifier that can be learned from positive-only data. The light purple line marks a classifier trained using the labeled vs unlabeled dataset, representing a naive baseline classifier.  The red line is the modified logistic regression model, which improves on the baseline.  The green line marks the multinomial logistic regression model which performs almost as well as knowing the true labels.}
\end{figure}

\end{document}